{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('./IMDB-Dataset.csv')\n",
    "#Removeduplicaterows\n",
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_14104\\1957807011.py:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text , \"html.parser\") # Removing html tags\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download ('stopwords')\n",
    "nltk.download ('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "stop = set( stopwords.words ('english'))\n",
    "# Expanding contractions\n",
    "def expand_contractions ( text ) :\n",
    "    return contractions . fix( text )\n",
    "# Function to clean data\n",
    "def preprocess_text ( text ) :\n",
    "    wl = WordNetLemmatizer ()\n",
    "    soup = BeautifulSoup(text , \"html.parser\") # Removing html tags\n",
    "    text = soup . get_text ()\n",
    "    text = expand_contractions ( text ) # Expanding chatwords and contracts clearing\n",
    "    contractions\n",
    "    emoji_clean = re. compile (\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"#emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"#symbols&pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"#transport&mapsymbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"#flags(iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\",flags=re.UNICODE)\n",
    "    text = emoji_clean . sub(r'',text )\n",
    "    text = re.sub (r'\\.(?=\\ S)', '. ',text ) #add space after full stop\n",
    "    text = re.sub (r'http \\S+', '', text ) # remove urls\n",
    "    text = \"\". join ([\n",
    "    word . lower () for word in text if word not in string . punctuation\n",
    "    ]) # remove punctuation and make text lowercase\n",
    "    text = \" \". join ([\n",
    "                    wl. lemmatize ( word ) for word in text . split () if word not in stop and word .\n",
    "                    isalpha () ]) # lemmatize\n",
    "    return text\n",
    "df['review'] = df['review'].apply(preprocess_text )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
